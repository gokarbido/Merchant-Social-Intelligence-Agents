![System Architecture](architecture.png)

# Merchant Social Intelligence Agents

## Agent Architecture Overview
This project implements a modular, multi-agent system for merchant social networks. Agents are orchestrated to classify, moderate, matchmake, and escalate messages, ensuring intelligent, safe, and relevant interactions.

### Core Agents
- **RouterAgent:** Classifies and routes messages to the appropriate agent.
- **MatchmakerAgent:** Recommends merchant connections based on profile, context, and (optionally) vector search.
- **ModeratorAgent:** Flags, warns, or allows messages based on LLM moderation.
- **HumanEscalationAgent:** Handles complex or high-risk cases by routing to a human operator.

Agents are orchestrated by a Pydantic-based orchestrator, with clear workflow tracking and feedback integration.

## How to Run Locally via Docker
You can run the full stack (API + Ollama LLM) using Docker or Docker Compose:

### Build and Run with Docker
1. Build the Docker image:
   ```bash
   docker build -t social-swarm .
   ```
2. Run the container:
   ```bash
   docker run -p 8000:8000 social-swarm
   ```
3. The API will be available at `http://localhost:8000`.

### Ollama Docker & Model Setup

This project uses [Ollama](https://ollama.com/) as the local LLM server. The Docker Compose setup will start an Ollama container for you, but you must manually pull the required model (e.g., `llama3.2`) the first time.

### How to Pull the Model
1. Start the stack with Docker Compose:
   ```bash
   docker-compose up -d ollama
   ```
2. Pull the model inside the running Ollama container:
   ```bash
   docker exec -it ollama ollama pull llama3.2
   ```
   Wait for the download to complete. You only need to do this once per model.
3. (Optional) Check available models:
   ```bash
   docker exec -it ollama ollama list
   ```

If the model is not pulled, requests to `/api/generate` will return 404 errors.
  
4. The Ollama will run at `http://localhost:11434`.

### PGVector Semantic Search (Vector Database)

This project uses [PGVector](https://github.com/pgvector/pgvector) as a vector database for semantic search and matchmaking. PGVector is enabled by default when running via Docker Compose.

- The `pgvector` service in `docker-compose.yml` runs a PostgreSQL database with the PGVector extension.
- The `social-swarm` service connects to this database using the `PGVECTOR_DSN` environment variable:
  ```
  PGVECTOR_DSN=postgresql://merchantuser:merchantpass@pgvector:5432/merchantdb
  ```
- The `MatchmakerAgent` will automatically use PGVector for semantic search if this variable is set.
- Embeddings are stored in the `merchant_embeddings` table with a `vector(384)` column.
- When a matchmaking request is made, the agent performs a similarity search using the `<->` operator to find the most relevant merchants.

#### Troubleshooting
- If you see an error like `vector type not found in the database`, connect to the `pgvector` container and run:
  ```sh
  docker exec -it pgvector psql -U merchantuser -d merchantdb -c "CREATE EXTENSION IF NOT EXISTS vector;"
  ```
- If you see an error about `operator does not exist: vector <-> numeric[]`, ensure your code is casting the embedding to `vector` (this is handled in the current codebase).

No additional setup is required for PGVector when using Docker Compose. If running outside Docker, ensure your PostgreSQL instance has the PGVector extension installed and enabled.

### Selecting a Vector Search Backend (PGVector, FAISS, ChromaDB)

You can choose which vector search backend to use for matchmaking by setting the `VECTOR_BACKEND` environment variable. Supported values:

- `pgvector` (default): Uses the PGVector database (recommended for production)
- `faiss`: Uses an in-memory FAISS index (fast, but not persistent)
- `chromadb`: Uses a persistent ChromaDB index (file-based)

**Embedding Model Setup:**
- The system uses Ollama for generating embeddings. You must pull the embedding model before use:
  ```sh
  docker exec -it ollama ollama pull all-minilm
  ```
- You can change the embedding model by setting the `OLLAMA_EMBEDDING_MODEL` environment variable.

**How to use:**

- To use FAISS:
  ```sh
  export VECTOR_BACKEND=faiss
  # or in Docker Compose, add to environment:
  # VECTOR_BACKEND=faiss
  ```
- To use ChromaDB:
  ```sh
  export VECTOR_BACKEND=chromadb
  # or in Docker Compose, add to environment:
  # VECTOR_BACKEND=chromadb
  ```
- If not set, defaults to `pgvector`.

**Note:**
- FAISS and ChromaDB do not require a database setup, but ChromaDB will create a `.chromadb` directory for persistence.
- PGVector is recommended for production and multi-instance deployments.

## How Agents Interact
- **User message** → **RouterAgent** (classifies intent)
  - If moderation needed → **ModeratorAgent** (may escalate to human)
  - If partnership/matchmaking needed → **MatchmakerAgent** (suggests connections)
  - If complex/unknown → **HumanEscalationAgent**
- Each step is recorded in the agent workflow for transparency.

## How Moderation & Matching Work
- **Moderation:**
  - LLM-based moderation flags or warns on abuse, spam, or low-quality content.
  - High-risk or unclear cases are escalated to a human operator.
- **Matching:**
  - MatchmakerAgent uses merchant profiles (and can use vector search) to suggest relevant partners.
  - Recommendations are context-aware and can be improved with feedback and embeddings.

## How Tests Are Structured
- **Unit tests:** Each agent has dedicated tests for core logic.
- **Scenario tests:** `tests/test_orchestrator_scenarios.py` covers real-world user messages and expected agent routing/behavior.
- **Run all tests:**
  ```bash
  pytest
  ```
Each test checks that the correct agent(s) handle the message and that the response and workflow match expectations.

## MCP Server/Client Integration Demo
- **MCP Server:**
  - Start the API (see Docker or local instructions).
  - `http://localhost:8000/mcp/message` endpoint accepts messages and feedback, returning agent workflow and response.
  - `http://localhost:8000/mcp/status` shows agent status and feedback memory.
- **MCP Client Example:**
  ```bash
  python mcp_client.py 011 "tenho interesse em fornecedores de roupas femininas" "thumbs-up"
  ```
- **Feedback Loop:**
  - User feedback is stored and can be used to improve agent decisions over time.

## Data
- Merchant profiles loaded from `fake_merchant_dataset.csv`


## LLM Output Variability & Test Robustness

This project uses LLMs (like Ollama/llama3.2) for agent decision-making. LLMs are non-deterministic and may return slightly different outputs for the same prompt. To ensure robust and realistic testing:

- **Tests are written to accept a range of valid responses** (e.g., different routing labels, escalation, or approval messages).
- **Assertions are loosened** to allow for synonyms, escalation, or fallback responses, reflecting real-world LLM behavior.
- **Moderation and matching tests** accept both strict and permissive outcomes, as LLMs may be more or less strict depending on context and prompt.

This approach mirrors best practices for production LLM systems, where output variability is expected and handled gracefully. If you want stricter or more predictable behavior, you can:
- Tune prompts for each agent
- Add post-processing or rules to filter/normalize LLM outputs
- Monitor and log unexpected outputs for future improvement

**Summary:** The system is robust, production-ready, and designed for real-world LLM usage, where variability is normal and handled by design.
